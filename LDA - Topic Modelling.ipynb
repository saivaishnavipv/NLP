{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "- Topic Modelling is a technique used to analyze the text data and discover and cluster text based on underlying semantic structures. It Falls under unsupervised learning as it does not require any predefined list of Tags/ ground truths to form a cluster\n",
    "\n",
    "### Topic modelling Algorithms\n",
    "\n",
    "- Latent Dirichlet Allocation (LDA)\n",
    "- Latent Semantic Analysis or Latent Semantic Indexing\n",
    "- Non-Negative Matrix Factorization\n",
    "\n",
    "LDA is the most preferred algorithm for Topic modelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "\n",
    "LDA works based on Matrix factoriztion technique. It assumes two matrices. \n",
    "- Document term matrix ( N,K)\n",
    "- Topic Term matrix (K, M)\n",
    "\n",
    "These Matrices correspond to <b>document word distribution & Topic word distribution</b> respectively\n",
    "\n",
    "### How LDA works ??\n",
    "- It Iterates through each word “w” for each document “d” and tries to adjust the current topic – word assignment with a new assignment.\n",
    "- A new topic “k” is assigned to word “w” with a probability P which is a product of two probabilities p1 and p2.\n",
    "- p1 = p(topic t / document d) =  percentage of words in a document d that are currently assigned to topic t.\n",
    "- p2 =  percentage of times the word w was assigned to topic t over all documents.\n",
    "- The current topic – word assignment is updated with a new topic with the probability, product of p1 and p2 . In this step, the model assumes that all the existing word – topic assignments except the current word are correct. This is essentially the probability that topic t generated word w, so it makes sense to adjust the current word’s topic with new probability.\n",
    "- After a number of iterations, a steady state is achieved where the document topic and topic term distributions are fairly good. This is the convergence point of LDA.\n",
    "\n",
    "### Parameters of LDA algorithm\n",
    "<b> Alpha and Beta Hyperparameters </b> – alpha represents document-topic density and Beta represents topic-word density. Higher the value of alpha, documents are composed of more topics and lower the value of alpha, documents contain fewer topics. On the other hand, higher the beta, topics are composed of a large number of words in the corpus, and with the lower value of beta, they are composed of few words.\n",
    "\n",
    "<b> Number of Topics</b> – Number of topics to be extracted from the corpus. Researchers have developed approaches to obtain an optimal number of topics by using Kullback Leibler Divergence Score. I will not discuss this in detail, as it is too mathematical. For understanding, one can refer to this[1] original paper on the use of KL divergence.\n",
    "\n",
    "<b>Number of Topic Terms</b> – Number of terms composed in a single topic. It is generally decided according to the requirement. If the problem statement talks about extracting themes or concepts, it is recommended to choose a higher number, if problem statement talks about extracting features or terms, a low number is recommended.\n",
    "\n",
    "<b>Number of Iterations / passes </b> – Maximum number of iterations allowed to LDA algorithm for convergence.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Topic Models\n",
    "- Refer to the below article for detailed explaination of How we evaluate topic models using \" topic coherence score\"\n",
    "- https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- https://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html\n",
    "- https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n",
    "- https://nlpforhackers.io/topic-modeling/\n",
    "- https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05\n",
    "- https://www.linkedin.com/pulse/lda-explanation-gaurhari-dass/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
